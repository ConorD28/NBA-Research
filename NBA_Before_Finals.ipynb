{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ConorD28/NBA-Research/blob/main/NBA_Before_Finals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rq6xsPJUvkXO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from sklearn.decomposition import PCA\n",
        "%matplotlib inline\n",
        "df = pd.read_csv('NBA Finals Research_Upload.csv')\n",
        "#df.drop(df.tail(37).index,\n",
        "#        inplace = True)\n",
        "df2 = df.drop(columns=df.columns[0], axis=1)\n",
        "df_NP = df2.drop(columns=df.columns[-25:], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.isnull().sum().sum() #Check if there are NA values"
      ],
      "metadata": {
        "id": "C8PEra5HfdD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f11438c-e3c9-4408-bca9-5e3406589e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LtJLaEVvqfY"
      },
      "outputs": [],
      "source": [
        "import scipy.stats\n",
        "def correlation(dataset, threshold, target): #Function to get Pearson's correlation between input and target\n",
        "  data = []\n",
        "  for i in range(len(dataset.columns)):\n",
        "      cor2 = dataset.iloc[:,i].corr(target) #scipy.stats.spearmanr(x, y)[0] and scipy.stats.kendalltau(x, y)[0] or Permutation Importance\n",
        "      #cor2 = scipy.stats.spearmanr(dataset.iloc[:,i], target[10:30])[0]\n",
        "      #cor2 = scipy.stats.kendalltau(dataset.iloc[:,i], target[10:30])[0]\n",
        "      column_headers = list(dataset.columns.values)\n",
        "      if(abs(cor2) > threshold):\n",
        "        data.append(dataset.iloc[:,i]) #make list of columns that meet the threshold\n",
        "        #if(abs(cor2) < .7):\n",
        "         # print(column_headers[i], f'cor:{cor2:.3f}')\n",
        "      i = i + 1\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "1xizqbXLDvrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngjdaqWIvssV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from numpy.random.mtrand import random_sample\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV, MultiTaskLassoCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5q3JY8L1e1M"
      },
      "outputs": [],
      "source": [
        "def Scores(y, y_pred, y_full):\n",
        "  MSE = mean_squared_error(y, y_pred)\n",
        "  MAE = mean_absolute_error(y, y_pred)\n",
        "  Normalized_RMSE = (np.sqrt(MSE)/np.mean(y_full))*100\n",
        "  Normalized_MAE = (MAE/np.mean(y_full))*100\n",
        "  Avg_Normalized_Score = (Normalized_RMSE + Normalized_MAE)/2\n",
        "  avg_error = (sum(abs(y_pred-y)))/10\n",
        "  print(f'Avg. Normalized Score:{ Avg_Normalized_Score:.1f}%')\n",
        "  print(f'Normalized RMSE:{ Normalized_RMSE:.1f}%')\n",
        "  print(f'Normalized MAE:{ Normalized_MAE:.2f}%')\n",
        "  #print(f'MAE:{ MAE:.3f}')\n",
        "  #print(f'RMSE:{ np.sqrt(MSE):.3f}')\n",
        "  print(f'Avg. Error:{avg_error:.1f}')\n",
        "  print(abs(y_pred-y))\n",
        "  return Avg_Normalized_Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLVl7ooxvt7q"
      },
      "outputs": [],
      "source": [
        "def RLE_Model(X, y, choice, predict_df): #Function to run Ridge, Lasso, or ElasticNet model\n",
        "  #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0) #Train/Test\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "\n",
        "  if(choice==\"Ridge\"):\n",
        "    alphas = np.geomspace(1e-10, 1e10, num=100)\n",
        "    m1 = RidgeCV(alphas=alphas)\n",
        "    m1.fit(X_train, y_train)\n",
        "    print()\n",
        "\n",
        "  if(choice==\"Lasso\"):\n",
        "    alphas = np.geomspace(1e-10, 1e10, num=100)\n",
        "    m1 = LassoCV(alphas=alphas)\n",
        "    m1.fit(X_train, y_train)\n",
        "    print(\"Lasso alpha: \", m1.alpha_)\n",
        "\n",
        "  if(choice==\"Elastic\"):\n",
        "    l1_ratio = [0, 0.3, 0.5, 0.7, 0.9, 1]\n",
        "    alphas = np.geomspace(1e-10, 1e10, num=100)\n",
        "    m1 = ElasticNetCV(alphas=alphas, l1_ratio=l1_ratio, max_iter=100000)\n",
        "    m1.fit(X_train, y_train)\n",
        "    print(\"Elastic alpha: \", m1.alpha_)\n",
        "\n",
        "  #print(f'Chosen alpha  {pipeline.steps[0][1].alpha_:.6f}')\n",
        "  #print(f'Intercept (b) {pipeline.steps[0][1].intercept_:.6f}')\n",
        "  #print(pd.Series(pipeline.steps[0][1].coef_, index=X.columns),'\\n')\n",
        "\n",
        "  #Calculate the predicted values:\n",
        "  y_train_pred = m1.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "  print()\n",
        "\n",
        "  y_test_pred = m1.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Test Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #Predict:\n",
        "  predictions = m1.predict(predict_df)\n",
        "\n",
        "  return y_test_pred, y_train_pred, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpU87Mgvvx0s"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def GBR_model(X,y, t, l, n, predict_df):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  reg = GradientBoostingRegressor(tol = t, learning_rate = l, n_estimators=n, random_state=0) #default: tol = 0.0001, learning rate - 0.1, 100, friedman_mse\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #Predict:\n",
        "  predictions = reg.predict(predict_df)\n",
        "  return y_test_pred, y_train_pred, predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "import xgboost as xgb\n",
        "import pickle\n",
        "\n",
        "def BR_model(X,y):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  reg = BaggingRegressor(base_estimator=xgb.XGBRegressor())\n",
        "  #reg = pickle.load(open(\"BR_model2\", \"rb\"))\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  #Predict:\n",
        "  #predictions = reg.predict(predict_df)\n",
        "  #print(predictions)\n",
        "\n",
        "  pickle.dump(reg, open(\"BR_model\", \"wb\"))\n",
        "  return y_test_pred, y_train_pred"
      ],
      "metadata": {
        "id": "ppib1skHfgGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfM5w0Ncvynu"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "#SGD Regressor:\n",
        "def SGD_model(X,y, t, ep, predict_df):\n",
        "\n",
        "  reg = SGDRegressor(max_iter=1000, tol=t, epsilon = ep) #tol = 0.001, epsilon=0.1\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  reg.fit(X_train, y_train)\n",
        "  y_train_pred = reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = reg.predict(X_test)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  predictions = reg.predict(predict_df)\n",
        "  #print(predictions)\n",
        "\n",
        "  pickle.dump(reg, open(\"SGD_model\", \"wb\"))\n",
        "  return y_test_pred, y_train_pred, predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Keras Sequential Neural Net\n",
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=2)\n",
        "\n",
        "def Keras_model(X,y,e, u, u2, u3, u4, u5, choice):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  model = Sequential()\n",
        "  model.add(Dense(u, input_dim=X_train.shape[1], activation='relu')) # Hidden 1, 60\n",
        "  model.add(Dense(units=u2,activation='relu')) # Hidden 2, 30\n",
        "  model.add(Dense(units=u3,activation='relu'))\n",
        "  model.add(Dense(units=u4,activation='relu'))\n",
        "  model.add(Dense(units=u5,activation='relu'))\n",
        "  model.add(Dense(units=15,activation='relu')) #15\n",
        "  model.add(Dense(units=1)) #,activation='relu'\n",
        "  model.compile(loss='mean_squared_error', optimizer=choice) #\n",
        "  model.fit(X_train, y_train, verbose=0, epochs=e, callbacks=[early_stop]); #callbacks=[early_stop]\n",
        "\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  y_train_pred = y_train_pred.flatten()\n",
        "\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  y_test_pred = y_test_pred.flatten()\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  #print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  model.save('/content/drive/MyDrive/Models/Keras_Model', save_format=\"h5\")"
      ],
      "metadata": {
        "id": "Zzt-o4jPgtdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "def DTR_model(X,y,leafs):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  # We introduce regularization by increasing the value of min_samples_leaf\n",
        "  tree_reg_regularized = DecisionTreeRegressor(random_state=42, min_samples_leaf=leafs)\n",
        "  tree_reg_regularized.fit(X_train, y_train)\n",
        "  y_train_pred = tree_reg_regularized.predict(X_train) #_regularized\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  y_test_pred = tree_reg_regularized.predict(X_test) #_regularized\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Predictions:\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()"
      ],
      "metadata": {
        "id": "sOg9t7czmLFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "def SVM_model(X,y,ep):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  svm_reg = LinearSVR(epsilon=ep, random_state=42) #default: epsilon = 0 tol=0.0001, C=1.0\n",
        "  svm_reg.fit(X_train, y_train)\n",
        "\n",
        "  #Train Predictions:\n",
        "  y_train_pred = svm_reg.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Test Predictions:\n",
        "  y_test_pred = svm_reg.predict(X_test)\n",
        "  print(\"Test predictions:\")\n",
        "  #print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()"
      ],
      "metadata": {
        "id": "luksQpOa9mx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "def SVM_models(X,y, choice, ep, C_value, predict_df):\n",
        "  X_train, X_test, y_train, y_test = X[10:30], X[0:10], y[10:30], y[0:10]\n",
        "  if(choice==\"rbf\"):\n",
        "    model = SVR(kernel=\"rbf\", C=C_value, gamma=0.1, epsilon=ep) #0.1 default ep; 100 default C, 0.1 default gamma\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  if(choice==\"poly\"):\n",
        "    model = SVR(kernel=\"poly\", C=C_value, gamma=\"auto\", degree=3, epsilon=ep, coef0=1) #0.1 default ep; 100 default C\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  if(choice == \"linear\"):\n",
        "    model = SVR(kernel=\"linear\", C=C_value, gamma=\"auto\", degree=3, epsilon=ep, coef0=1) #0.1 default ep; 100 default C\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "  #Train Predictions:\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  #print(y_train_pred)\n",
        "\n",
        "  #Training Scores:\n",
        "  Avg_N_Score_train = Scores(y_train, y_train_pred, y)\n",
        "  #print()\n",
        "\n",
        "  #Test Predictions:\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  print(\"Test predictions:\")\n",
        "  print(y_test_pred)\n",
        "  #print()\n",
        "\n",
        "  #Testing Scores:\n",
        "  Avg_N_Score_test = Scores(y_test, y_test_pred, y)\n",
        "  print(f'Difference of avg scores:{ Avg_N_Score_test - Avg_N_Score_train:.2f}%') #Difference between testing and traing scores to check if my bias-variance tradeoff is good\n",
        "  print()\n",
        "\n",
        "  final_preds = model.predict(predict_df)\n",
        "\n",
        "  return y_test_pred, y_train_pred, final_preds"
      ],
      "metadata": {
        "id": "1ms2SSQ_3ncH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "from sklearn.manifold import LocallyLinearEmbedding"
      ],
      "metadata": {
        "id": "UhxkZpeNTZA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.iloc[:, 1110:]"
      ],
      "metadata": {
        "id": "GQ1MImLOcqjR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Importance:\n",
        "scaler = StandardScaler() #MinMaxScaler, StandardScaler\n",
        "X_train, X_test, y_train = df_NP[10:30], df_NP[0:10], df[10:30]\n",
        "data_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "\n",
        "data_scaled.drop([\"SOS\", \"SOS_Use\", \"EFGPercPerSOS_Per 100_Clutch_Plr\", \"FGPercPerSOS_Per 100_Clutch_Plr\",\n",
        "                  \"FGPerc_5_9 FT._Last10.1\", \"PACE_Per 100_Plr\", \"ROAD SOS\", \"FGPercPerSOS_10_14 FT._Last10\",\n",
        "                  \"FGAPerSOS_5_9 FT._Last10\", 'FGPercPerSOS_Above the Break 3', 'PACE_Per 100_vsOwnConf_Plr',\n",
        "                  'FGPerc_Right Corner 3', 'FGMPerSOS_Mid Range', 'FGPercPerSOS_20_24 FT._vsOwnC',\n",
        "                  'WSPer48PerSOS_Per 100_Plr', 'EFGPercPerSOS_Per 100_vsOwnConf_Plr',\n",
        "                  'TD3_Per 100_Clutch_Plr', 'FGAPerSOS_10_14 FT.', 'FGPercPerSOS_10_14 FT.',\n",
        "       'FGPercPerSOS_20_24 FT.'],axis=1,inplace = True) #dropped stats for EST DEF RTG\n",
        "\n",
        "data_correlated = correlation(data_scaled, 0.561, y_train['EST_DEFRTG_Pfs']) #\n",
        "data_correlated_df = pd.DataFrame(data_correlated)\n",
        "data_correlated_df2 = data_correlated_df.transpose() #Correlated inputs\n",
        "\n",
        "#Variables Added to EST Def Rtg:\n",
        "data_correlated_df2.insert(21,\"EST. DEFRTG\",data_scaled[\"EST. DEFRTG\"])\n",
        "data_correlated_df2.insert(22,\"OPP EFGPercPerSOS_PerGM\",data_scaled[\"OPP EFGPerc_PerGM\"])\n",
        "data_correlated_df2.insert(23,\"OPP TOVPerc_PerGM\",data_scaled[\"OPP TOVPerc_PerGM\"])\n",
        "data_correlated_df2.insert(24,\"OPP FBPSPerSOS_Per 100\",data_scaled[\"OPP FBPSPerSOS_Per 100\"])\n",
        "data_correlated_df2.insert(25,\"OPP PITPPerSOS_Per 48_Clutch\",data_scaled[\"OPP PITPPerSOS_Per 48_Clutch\"])\n",
        "data_correlated_df2.insert(26,\"OPP PITP PerSOS_Per 100\",data_scaled[\"OPP PITP PerSOS_Per 100\"])\n",
        "data_correlated_df2.insert(27,\"PP100PerSOS_LA\",data_scaled[\"PP100PerSOS_LA\"])\n",
        "\n",
        "X = df_NP.loc[:, data_correlated_df2.columns] #get non scaled data with important features\n",
        "\n",
        "#Train test split and scale:\n",
        "X_train, X_test = X[10:30], X[0:10] #split non-scaled important variables\n",
        "X_train_processed = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
        "X_test_processed = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
        "correlated_scaled_data = pd.merge(X_test_processed, X_train_processed, how = 'outer')\n",
        "#correlated_scaled_data_MMS = pd.merge(X_test_processed, X_train_processed, how = 'outer')\n",
        "\n",
        "#PCA:\n",
        "pca=PCA(n_components = 2)\n",
        "X_train_processed_PCA = pca.fit_transform(X_train_processed)\n",
        "X_train_PCA_df = pd.DataFrame(X_train_processed_PCA)\n",
        "X_test_processed_PCA = pca.transform(X_test_processed)\n",
        "X_test_PCA_df = pd.DataFrame(X_test_processed_PCA)\n",
        "data_PCA = pd.merge(X_test_PCA_df, X_train_PCA_df, how = 'outer')\n",
        "#data_PCA_MMS = pd.merge(X_test_PCA_df, X_train_PCA_df, how = 'outer')\n",
        "\n",
        "print(\"Principal axes:\\n\", pca.components_.tolist())\n",
        "print(\"Explained variance:\\n\", pca.explained_variance_.tolist())\n",
        "print(\"Mean:\", pca.mean_)\n",
        "\n",
        "#LLE:\n",
        "#X_swiss, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=19, random_state=42) #n_components=2 is default, neighbors 5 is default\n",
        "X_unrolled_train = lle.fit_transform(X_train_processed)\n",
        "X_train_LLE_df = pd.DataFrame(X_unrolled_train)\n",
        "X_unrolled_test = lle.transform(X_test_processed)\n",
        "X_test_LLE_df = pd.DataFrame(X_unrolled_test)\n",
        "data_LLE = pd.merge(X_test_LLE_df, X_train_LLE_df, how = 'outer')\n",
        "#data_LLE_19 = pd.merge(X_test_LLE_df, X_train_LLE_df, how = 'outer')\n",
        "#data_LLE_MMS = pd.merge(X_test_LLE_df, X_train_LLE_df, how = 'outer')\n",
        "#data_LLE_MMS_19 = pd.merge(X_test_LLE_df, X_train_LLE_df, how = 'outer')\n",
        "\n",
        "X = correlated_scaled_data #try correlated_scaled_data, data_PCA, data_LLE\n",
        "y = df2['EST_DEFRTG_Pfs'] #"
      ],
      "metadata": {
        "id": "Z0RXs3FQMoHz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropped stats for EST OFF RTG\n",
        "data_scaled.drop([\"NETRTGPerSOS_Per 100_Clutch_Plr\", \"DEFRTG_Per 100_Clutch_Plr\",\n",
        "                  \"NETRTG_Per 100_Clutch_Plr\", \"DREBPerSOS_Per 100_Clutch_Plr\",\n",
        "                  \"DREB_Per 100_Clutch_Plr\", '+Per_PerSOS_Per 100_Clutch_Plr',\n",
        "                  'DD2PerSOS_Per 100_Clutch_Plr', \"+Per__Per 100_Clutch_Plr\",\n",
        "                  'OPP PTSPerSOS_Per 100_Clutch', 'oFGA_Above the Break 3', 'Seed',\n",
        "                  'DREBPerSOS_Per 100_Clutch_Plr', 'oFGA_20_24 FT._Last10'],axis=1,inplace = True)\n",
        "\n",
        "#Variables Added to EST Off Rtg:\n",
        "data_correlated_df2.insert(26,\"PTS_Per 48\",data_scaled[\"PTS_Per 48\"])\n",
        "data_correlated_df2.insert(27,\"EST. OFFRTGPerSOS\",data_scaled[\"EST. OFFRTGPerSOS\"])\n",
        "data_correlated_df2.insert(28,\"FGMPerSOS_Corner 3\",data_scaled[\"FGMPerSOS_Corner 3\"])\n",
        "data_correlated_df2.insert(29,\"PP100PerSOS_LA\",data_scaled[\"PP100PerSOS_LA\"]) #\n",
        "data_correlated_df2.insert(30,\"eFG+_Plr\",data_scaled[\"eFG+_Plr\"]) #\n",
        "data_correlated_df2.insert(31,\"FGPerc_10_14 FT._Last10.1\", data_scaled[\"FGPerc_10_14 FT._Last10.1\"])\n",
        "data_correlated_df2.insert(32,\"AST RATIO_Per 100_vsOwnConf_Plr\", data_scaled[\"AST RATIO_Per 100_vsOwnConf_Plr\"])\n",
        "data_correlated_df2.insert(33,\"FTAPerSOS_Per 100_Clutch\", data_scaled[\"FTAPerSOS_Per 100_Clutch\"])\n",
        "data_correlated_df2.insert(34,\"TSPerc_Per 48_Clutch\", data_scaled[\"TSPerc_Per 48_Clutch\"])\n",
        "data_correlated_df2.insert(35,\"FGA_<5 FT.\",data_scaled[\"FGA_<5 FT.\"])\n",
        "data_correlated_df2.insert(36,\"AST_Per 100_vsOwnConf_Plr\",data_scaled[\"AST_Per 100_vsOwnConf_Plr\"]) #\n",
        "data_correlated_df2.insert(37,\"FGA_RA\",data_scaled[\"FGA_RA\"]) #eFG+_Plr\n",
        "data_correlated_df2.insert(38,\"ASTPerTO_Per 48_Clutch\",data_scaled[\"ASTPerTO_Per 48_Clutch\"]) #\n",
        "data_correlated_df2.insert(39,\"AST Ratio_Per 100 Plays_vsOwnC\",data_scaled[\"AST Ratio_Per 100 Plays_vsOwnC\"]) #"
      ],
      "metadata": {
        "id": "3WBSwlY1YrkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model: .6\n",
        "RLE_Model(X, y, \"Ridge\", X) #\n",
        "RLE_Model(X, y, \"Lasso\", X) #\n",
        "SGD_model(X,y, 1e-3, 0.1, X) #\n",
        "GBR_model(X,y, .0001, 0.01, 100, X) #\n",
        "print(\"5:\")\n",
        "DTR_model(X,y, 100) #default is 100"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EXAu8bIGJuDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_models(X, y, \"rbf\", 3, 100, X) #3.7%, 1.9%, corr_MMS 2.3\n",
        "SVM_models(X, y, \"poly\", 2.2, 100, X) #3.9%, 1.1%, LLE_19 4.5, 3.9%, 1.5%, PCA_MMS 1.9, 3%, 1.3%, corr_MMS 2.1\n",
        "SVM_models(X, y, \"linear\", 2.9, 100, X) #3.2%, .9%, corr, 3.6%, .5%, PCA 5.1, 3.8%, .6%, PCA_MMS 5.1, 3.2%, .9%, corr_MMS\n",
        "SVM_model(X,y, 0) #"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h1UNKDfyoVsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVM_models(X, y, \"poly\", 3.2, 100, X)\n",
        "SVM_models(X, y, \"linear\", 1.9, 100, X)\n",
        "SVM_models(X, y, \"poly\", 4.2, 100, X)\n",
        "SVM_models(X, y, \"linear\", 3.9, 100, X)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gTNnR1hznn54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RLE_Model(X, y, \"Elastic\", X) #\n",
        "BR_model(X,y) #\n",
        "Keras_model(X, y, 200, 120, 60, 30, 20, 15, \"adamax\") #\n",
        "#adam, nadam; adamax"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dAlCvr6DJ06E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Estimated** **Ratings**"
      ],
      "metadata": {
        "id": "5shDpDOhXFI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EST_OFFRTG_Pfs Blender: #3.1%, 1%, 3.0% avg error, .6\n",
        "est_offrtg = pd.read_csv('EST_OFFRTG Inputs.csv')\n",
        "scaler = StandardScaler()\n",
        "train_processed = scaler.fit_transform(X_train)\n",
        "scaled_data = scaler.transform(est_offrtg)\n",
        "scaled_data_df = pd.DataFrame(scaled_data)\n",
        "\n",
        "scaler_MMS = MinMaxScaler()\n",
        "train_processed_MMS = scaler_MMS.fit_transform(X_train)\n",
        "scaled_data_MMS = scaler_MMS.transform(est_offrtg)\n",
        "scaled_data_MMS_df = pd.DataFrame(scaled_data_MMS)\n",
        "\n",
        "X = correlated_scaled_data\n",
        "X_corr_MMS = correlated_scaled_data_MMS\n",
        "\n",
        "preds = SVM_models(X_corr_MMS, y, \"poly\", 2.1, 100, scaled_data_MMS_df) #3%, 1.3%, corr_MMS\n",
        "preds2 = SVM_models(X, y, \"linear\", 2.9, 100, scaled_data_df) #3.2%, .9%, corr\n",
        "preds3 = SVM_models(X_corr_MMS, y, \"linear\", 2.9, 100, scaled_data_MMS_df) #3.2%, .9%, corr_MMS\n",
        "\n",
        "#Get stats on blender\n",
        "train_preds = (preds[1] + preds2[1] + preds3[1])/3\n",
        "\n",
        "test_preds = (preds[0] + preds2[0] + preds3[0])/3\n",
        "\n",
        "print(\"Blender Train Scores then Test Scores:\")\n",
        "Scores(y[10:30], train_preds, y)\n",
        "print()\n",
        "Scores(y[0:10], test_preds, y)\n",
        "print()\n",
        "\n",
        "est_off_rtg_predictions = pd.DataFrame()\n",
        "est_off_rtg_predictions[\"Predictions\"] = (preds[2] + preds2[2] + preds3[2])/3\n",
        "#est_off_rtg_predictions.to_excel(\"EST_OFF_RTG_Predictions.xlsx\")"
      ],
      "metadata": {
        "id": "K2ZPYeuumlm5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "est_off_rtg_predictions[\"Predictions\"]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "98ksmkqKdRjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EST_DEFRTG_Pfs Blender: #2.3%, .8%, 2.2% avg error, .561\n",
        "est_def_rtg = pd.read_csv('EST_DEFRTG Inputs.csv')\n",
        "scaler = StandardScaler()\n",
        "train_processed = scaler.fit_transform(X_train)\n",
        "scaled_data = scaler.transform(est_def_rtg)\n",
        "scaled_data_df = pd.DataFrame(scaled_data)\n",
        "\n",
        "lle.fit_transform(X_train)\n",
        "scaled_data_LLE = lle.transform(scaled_data)\n",
        "scaled_data_LLE_df = pd.DataFrame(scaled_data_LLE)\n",
        "\n",
        "scaler_MMS = MinMaxScaler()\n",
        "scaler_MMS.fit_transform(X_train)\n",
        "scaled_data_MMS = scaler_MMS.transform(est_def_rtg)\n",
        "scaled_data_MMS_df = pd.DataFrame(scaled_data_MMS)\n",
        "\n",
        "X = correlated_scaled_data\n",
        "#X_PCA = data_PCA\n",
        "X_corr_MMS = correlated_scaled_data_MMS\n",
        "#X_PCA_MMS = data_PCA_MMS\n",
        "X_LLE_19 = data_LLE_19\n",
        "#X_LLE_19_MMS = data_LLE_MMS_19\n",
        "\n",
        "loaded_BR_model = pickle.load(open(\"BR_model_LLE_DEF\", \"rb\")) #\n",
        "\n",
        "preds = (loaded_BR_model.predict(X_LLE_19[0:10]), loaded_BR_model.predict(X_LLE_19[10:30]), loaded_BR_model.predict(scaled_data_LLE)) #1.7%, .8% (21), LLE_19\n",
        "preds2 = SVM_models(X, y, \"linear\", 3.5, 100, scaled_data_df) #3.1%, .8% , corr\n",
        "#preds3 = SVM_models(X_corr_MMS, y, \"linear\", 3.5, 100, scaled_data_MMS_df) #3.2%, .9%, corr_MMS\n",
        "#preds4 = SVM_models(X_PCA_MMS, y, \"linear\", 4.7, 100, X_PCA_MMS) #3.2%, .6% (35), PCA_MMS\n",
        "\n",
        "#Get stats on blender\n",
        "train_preds = (preds[1] + preds2[1])/2\n",
        "\n",
        "test_preds = (preds[0] + preds2[0])/2\n",
        "\n",
        "print(\"Blender Train Scores then Test Scores:\")\n",
        "Scores(y[10:30], train_preds, y)\n",
        "print()\n",
        "Scores(y[0:10], test_preds, y)\n",
        "print()\n",
        "\n",
        "est_def_rtg_predictions = pd.DataFrame()\n",
        "est_def_rtg_predictions[\"Predictions\"] = (preds[2] + preds2[2])/2\n",
        "#est_def_rtg_predictions.to_excel(\"EST_DEF_RTG_Predictions.xlsx\")"
      ],
      "metadata": {
        "id": "UlBjGv23VyoP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "est_def_rtg_predictions[\"Predictions\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRbWXgw1T3gR",
        "outputId": "cbd8f414-5c62-41c7-a8be-89591feec007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    98.156553\n",
              "1    98.668559\n",
              "Name: Predictions, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Vm8WmlMWl5qPkDh4nqbzn64vOM6ndqxq",
      "authorship_tag": "ABX9TyPq9DWiiwYRFPwiC+oypzn1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}